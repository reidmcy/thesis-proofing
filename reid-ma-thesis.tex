\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{mathtools}
\usepackage{titling}
\usepackage[hidelinks]{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{pbox}
\usepackage{adjustbox}
\usepackage{MnSymbol}
\usepackage{wasysym}
\usepackage{geometry}
\usepackage{ragged2e}
\usepackage[group-separator={,}, round-mode = places, round-precision = 2]{siunitx}

\usepackage{setspace}
%\doublespacing

\usepackage {tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes.misc, positioning, shapes.geometric}

\usepackage[font={it}, labelfont=bf]{caption}
\usepackage{graphicx}

\graphicspath{{images/}}

\author{Reid McIlroy-Young}
\title{Masters in Computational Social Science}
\date{May 1, 2018}

\setcounter{tocdepth}{2}

\begin{document}
\pagenumbering{gobble}
\maketitle
\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage
\doublespacing
\setcounter{page}{1}
\pagenumbering{arabic}


\section{Introduction}

The mass proliferation of micro computers that began last century has had, and continues to have an incredible impact on society \citep{weizenbaum1972impact}, the economy \citep{gordon2000does} and information distribution \citep{berners2010world}. The access to computers -- the scale, quality and depth of data they provide, has also had major effects on science \citep{lazer2009life}, but analysis has been primarily focused on either the macro cultural effects \citep{pfaffenberger1988social} or the economic usage \citep{landauer1995trouble}.

By comparison the usage of computers by scientists has been overlooked by researchers. The impact of computers has spread, at different rates, throughout all the domains of science. The social sciences have been one of the last to be significantly effected, but this is likely to change this century \citep{watts2007twenty}. There are large datasets of medical, social and economic data generated by a complex web of automated systems whose implications we are not yet able to interpret at more than a service level \citep{kossinets2006empirical} \citep{back2010emotional}, with many examples of even these surface level analyses failing \citep{lazer2014parable} \citep{kramer2014experimental}. One aspect of this `paradigm shift' that has had much less scrutiny is how the individual scientists are interacting with these new computational tools. Are social scientists still able to do good work with the traditional techniques? Or are computers going to eat everything?

One of the main methods for large scale analysis of the culture or structure of scientific work involve bibliometric techniques \citep{de2009bibliometrics} using large standard datasets \citep[e.g.][]{Boyack2005, borner2010atlas, borner2015atlas, sugimoto2013global, shi2015weaving, evans_meta, skupin2013visualizing}. These dataset are generally lacking information about the computational aspects of the work, e.g. the  Clarivate Analytics Web of Science (WOS) does not have any such field \citep{mkdocs} and as such research into this dimension is difficult. Recent developments in natural language processing (NLP) have shown that complex concepts can be extracted reliably from text for a wide variety of tasks \citep{evans2016machine}, with some very similar to that done here \citep{foster2015tradition}.

Large scale surveys of science to determine computational usage have not been done, although much has been written about how computers or other computationally derived methods will revolutionize science \citep[e.g.][]{de1997computer, anderson2008end,  provost2013data, john2014big}. We can infer from the proliferation of new computational journals, e.g. Journal of Computational Social Science first issue published in January 2018, that there is a demand for computational knowledge and techniques though science, although the extent and means of spread is not well characterized, which this paper hopes to work towards correcting. Once the usages of computation can be identified there is large number of existing frameworks in which the diffusion \citep{griliches1960hybrid}, spread \citep{padgett1993robust} or innovation \citep{foster2015tradition} of computational methods can be viewed.

In this paper I am using an bidirectional Long Short Term Memory (LSTM) based classifier \cite{graves2005framewise}, that I have been developing, for extracting useful classifications with only partially classified data. The specific task is to identify the description of computational style/techniques/approaches in journal articles, talks and other publications outside of the explicitly computational publications, conferences or other sources. The paper will seek to both quantify the level computational publication in the social sciences and discuss their implications.


\section{Methodology}

\subsection{Data}

The source of data used for this analysis is the  Web of Science (WOS) database hosted by \textit{Knowledge Lab}. The data for the database was collected by \textit{Thompson Reuters} until 2016 , when it was transferred to \textit{Clarivate Analytics} who now maintain it \citep{clarivate}. It has metadata on almost all scientific publications from 1960 to 2015, with new records being more complete. The data I used are a subset of this larger collection, specifically  the \num{1457418} publications with a Social Science subject classification from 2005 to 2015. The classifications are derived from the publication's journal or conference (source) \citep{kottawos} and are divided into six subjects (e.g. Natural Sciences or Social Sciences), then further dived into subject categories (e.g. Cultural Studies or Ergonomics). Each source has one or more subject categories, which are done manually and generally overlap with other classifications \citep{efremenkova2016comparison}. The source's classifications can be passed onto their publications and thus I can assign publication to one or more disciplines and  sub-disciplines within a well defined hierarchy \citep{wossubjects}. A summary of the data is shown in Table \ref{sum_1}.

To derive information about the computational nature of the publications, the `Computer and information sciences' sub-discipline was used to identify the \num{11115} sources from 2005 to 2017 with explicitly computational subjects and found the intersection of those and the social science journals, giving \num{782} sources and \num{106680} publications that are explicitly computational in nature. Table \ref{sum_comp} gives the breakdowns for each subject, notably the percentage of explicitly computational works varies by a factor of \num{50}, from \num{0.77}\% in Law to \num{38}\% in Media and Communication. 

In addition to the subject tags, the abstracts, titles, authors, publication month, publication year and unique identification number (\textit{WOS ID}) were gathered and are used later in the analysis.

An additional dataset used for comparisons is the Stack Exchange Data Dump, collection of user created data from the \href{http://stackexchange.com/}{Stack Exchange network}, hosted by the \href{https://archive.org/details/stackexchange}{Internet Archive} under a creative commons attribution share alike 3.0 license \citep{StackExchange}. The Stack Exchange network is collection of question and answer websites siloed by category. In this case the Stack Overflow (general programming questions), Economics and Psychology \& Neuroscience sites are of interest. What is used from the dump is the user generated tags; each question is assigned one or more (usually three) tags with tags being generated by high ranking community members.

\begin{table}[h]
	\centering
	\begin{adjustbox}{center}
		\begin{tabular}{lrrrl}
			\toprule
			{} &  \pbox{20cm}{Number of \\Publications}  &  \pbox{20cm}{Number of\\Sources}&  \pbox{20cm}{Number of \\Subjects} &   \pbox{20cm}{Example Subject\\Categories} \\
			\midrule
			Economics and Business  &            \num{459992} &              \num{2363} &                  \num{6} &            Business \\
			Psychology              &            \num{366313} &              \num{1017} &                 \num{11} &          Ergonomics \\
			Educational Sciences    &            \num{168342} &              \num{1009} &                  \num{3} &  Education, Special \\
			Sociology               &            \num{136894} &               \num{777} &                  \num{9} &           Sociology \\
			Political Science       &             \num{95920} &               \num{609} &                  \num{3} &   Political Science \\
			Other Social Sciences   &             \num{83611} &               \num{571} &                  \num{4} &       Asian Studies \\
			Media and Communication &             \num{63625} &               \num{518} &                  \num{2} &       Communication \\
			Law                     &             \num{40829} &               \num{295} &                  \num{2} &                 Law \\
			\midrule
			Full Data set           &           \num{1457418} &              \num{6893} &                 \num{46} &                     \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Summary of used data, note publications can have multiple subjects, thus the final row is not a sum}\label{sum_1}
\end{table}

\begin{table}[h]
	\centering
	\begin{adjustbox}{center}
		\begin{tabular}{lrrl}
			\toprule
			{} &  \pbox{20cm}{Computational\\ Publications} &  \pbox{20cm}{Percentage\\Explicitly Computational} &  \pbox{20cm}{Example of Explicitly\\Computational Source} \\
			\midrule
			Economics And Business  &           \num{60602} &        \num{13.174577} &    Decision Support Systems \\
			Psychology              &            \num{5364} &         \num{1.464321} &  Interacting With Computers \\
			Educational Sciences    &           \num{17988} &        \num{10.685390} &       Computers \& Education \\
			Sociology               &            \num{3975} &         \num{2.903707} &       Persuasive Technology \\
			Political Science       &            \num{1815} &         \num{1.892202} &       Electronic Government \\
			Other Social Sciences   &            \num{2829} &         \num{3.383526} &           Adaptive Behavior \\
			Media And Communication &           \num{24798} &        \num{38.975246} &              Scientometrics \\
			Law                     &             \num{313} &         \num{0.766612} &    Law And The Semantic Web \\
			\midrule
			Full Data Set           &          \num{106680} &         \num{7.319794} &   \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Distribution of explicitly computational publications, note publications can have multiple subjects, thus the final row is not a sum}\label{sum_comp}
\end{table}


\subsection{Preprocessing}

The publication year and month fields are given as integers so required no processing to use. While the other fields are are all raw text, both the title and abstract in particular needed to be tokenized (the words separated from each other) and the word embeddings constructed. The tokenizing was done with the \textit{Natural Language Toolkit}'s (\textit{NLTK}) \citep{bird2006nltk} English language tokenizers, specifically sentence tokenizing was done with the Punkt system \citep{kiss2006unsupervised} and the words tokenized with a collection of regular expresions derived from the Penn Treebank \citep{marcus1993building}. The other fields were treated as raw text, without any additional transformations.

The final step in preparing the data for the neural network was constructing the word embeddings. A word embeddings is a mapping from a collection of words to a collections of vectors, in a high dimensional vector space with similar words closer together in the space\citep{wordembeding}. The method used is \textit{Word2Vec} \citep{mikolov2013efficient} as implemented by \textit{gensim} \citep{rehurek_lrec}. \textit{Word2Vec} gives words vectors whose dimensions can be interpreted as a collection of semantic directions, e.g. one direction might be the masculine-feminine dimension of the word while another might be the `computationalness' of the word \citep{bolukbasi2016man}. This relationship derives from this type of neural auto-encoder/embedding being an approximation of SVD matrix factorization \citep{levy2014neural} that minimizes the pointwise mutual information, thus the space has many of the properties of a Euclidean Hilbert space. Unfortunately determining the meaning of the dimensions is functionally impossible, due to their number (in this case 200) and their non-linear relationships (the masculine-feminine direction, if it exists in the particular embedding, is a weighted combination of all 200 basis directions and is likely not orthogonal `computationalness'). The embedding is still very useful as it gives similar words similar vectors and makes their divergences depend on the meanings, e.g. king and queen diverge in similar ways to man and woman. The word embeddings act as a foundation for the deep neural network to build on and are the standard first step \citep{deep_learning_chapter12} as it converts variable length strings into fixed length vectors. For all embeddings hierarchical softmax was used with a window size of five, 200 dimensional output, five iterations and no removal of words for any reason. This configuration should allow for infrequent words to be embedded nearly optimally  \citep{rehurek_lrec}, but there is likely room for improvement.

The alternative to an auto-encoder is a one-hot vector representation where each word is a dimension of the input vector and all but one of the values is zero, with the non-zero element (usually equal to one) corresponding to the selected word's dimension. This method greatly increases the size of the first layer as in this case there are \num{11430321} unique words in the combined corpus and means the RNN has to learn the meaning of each word separately as the location in the vector space provides no additional information. Thus these methods are most commonly used for much simpler inputs, e.g. character level tasks \citep{rodriguez1999recurrent}.
 
 \subsection{Processing}

The classification of the publications was done with two separate, two layer bidirectional \citep{graves2013hybrid} Long Short Term Memory (LSTM) \citep{Hochreiter:1997:LSM:1246443.1246450} Recurrent Neural Networks (RNNs), with the LSTM implementation used is \textit{NVIDIA}'s \textit{cuDNN} \citep{chetlur2014cudnn}, see Figure \ref{lstm} for an illustration. LSTM cells are a variant of RNN cells, which work by taking in two vectors, the input vector (in this case the word2vec vector for a word) and the recurrent vector which is initially all zeros. These two vectors are append together into a single input which is fed to a single layer neural network whose output layer has two components, the required output (in this case the classification) and vector the same size as the input recurrent vector. The output recurrent vector is then fed to the cell along with the next item in the sequence, thus giving a stateful component to the next output. Unfortunately the memory of the cell tends to have short life span \citep{greff2017lstm} as every new input is combined with the memory vector. The LSTM architecture solves this by adding a second output vector, the blue lines in Figure \ref{lstm}. This second vector acts indirectly on the input and is in turn indirectly modified, via the peepholes, by the LSTM cell each cycle. This configuration means the \textit{long term} memory vector is modified much less by each input and thus retains more of the history. The weights of the cell are trained by backpropagation\citep{pytorch} just like most other neural network architectures, and the specifics of which have been thoroughly explored\citep{riedmiller1993direct}.

\begin{figure}[ht]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[width=1\textwidth]{LSTM.png}
	\end{adjustbox}
	\caption{Illustration of LSTM cell as implemented by \textit{NVIDIA}\\Image by Klaus Greff and colleagues as published in LSTM: A Search Space Odyssey \citep{greff2017lstm}}\label{lstm}
\end{figure}

To create the classifier used in this paper 16 LSTM cells were used, their arrangement is shown in Figure \ref{rnn}, which shows the network's structure and the flow of data through the model. Starting from the bottom the abstract and title are provided as raw text, the they go through the two preprocessing steps, tokenizing and embedding, then result of which is each word becomes a vector $\vec{x}_t = \begin{bmatrix}0&.2&0&\dots&.3&0\end{bmatrix}^T$. The process is identical, modulus weights, for the title and abstract, for this example we will consider the title. The word's vector is fed to the first forward layer ($h_t^1$) of the RNN, which applies the non-linear transforms described above, before finally outputting three $128$ dimensional vectors (\textit{long term}, \textit{short term} and output vectors). The output is fed to the next layer ($h_t^1 \rightarrow h^2_t$) while the memory vectors are given back to the cell along with the next word vector ($h_t^1 \rightarrow h^1_{t+1}$). The next layer repeats the process with the out put being ignored until the final word is provided. These ignored outputs can be of use and will be discussed. Simultaneously to this the last word is provided to the first reverse layer  ($g^1_t$) where the same process as the forward layer takes place, the output is fed up the network ($g_t^1 \rightarrow g^2_t$) while the memory vectors go to the next (previous word in the text) word  ($g_t^1 \rightarrow g^2_{t+1}$). The final output vectors ($128$ dimensional) from the final word in the title in the forward direction along with the final word from the reverse backwards are appended to each other to create one vector ($\oplus$), a 256 dimensional vector giving the module's `thoughts' on what was in the title. This vector is then combined with the output from the abstract and given to a final neural network ($u$) that looks at the two combined 512 dimensional vector and produces a single 2 dimensional vector ($\vec{y}$). The two dimensions provided are the log-odds of the publication \textit{not} being computational and the log-odds of the publication being computational.

When first run on a publication the neural network's weights, the parameters for the non-linear transforms, are randomly selected, so it's outputs are useless. Thus it has to be trained. To train the model a training data set is created and labelled with the correct outputs for each of the publications in the set. Then publications are randomly selected from the set to fed through the network and their outputs compared to the expected outputs. Through use of backpropagation \citep{werbos1982applications} (differentiating the output with respect to every parameter and every input) the given output and correct output are compared and the network's weights are updated to moving future inputs more in line with the correct outputs. The networks is then tested for accuracy against separate set of publications once an epoch (in this case 500 training examples), the training set, to see if it is making accurate predictions.

\begin{figure}[H]
	\centering
	
	\begin{tikzpicture}[shorten >=1pt,auto,node distance=2cm, very thick]
	\node[draw, rounded corners] (1) {Abstract};
	\node[draw, rounded corners] (tokenizer) [above = 1cm of 1] {Tokenizer};
	\node[draw, rounded corners] (2) [above = 1cm of tokenizer] {word2vec};
	\node[draw] (3) [above of=2] {$\vec{x}_{t}$};
	\node[draw] (4) [left of =3] {$\vec{x}_{t-1}$};
	\node[draw] (5) [right of =3] {$\vec{x}_{t+1}$};
	
	\node[draw, circle] (h1) [above of=3] {$h_t^1$};
	\node[draw, circle] (h2) [above of=4] {$h_{t-1}^1$};
	\node[draw, circle] (h3) [above of=5] {$h_{t+1}^1$};
	
	\node[draw, circle] (g1) [above of=h1] {$g_t^1$};
	\node[draw, circle] (g2) [above of=h2] {$g_{t-1}^1$};
	\node[draw, circle] (g3) [above of=h3] {$g_{t+1}^1$};
	
	\node[draw, circle] (h12) [above of=g1] {$h_t^2$};
	\node[draw, circle] (h22) [above of=g2] {$h_{t-1}^2$};
	\node[draw, circle] (h32) [above of=g3] {$h_{t+1}^2$};
	
	\node[draw, circle] (g12) [above of=h12] {$g_t^2$};
	\node[draw, circle] (g22) [above of=h22] {$g_{t-1}^2$};
	\node[draw, circle] (g32) [above of=h32] {$g_{t+1}^2$};
	
	\node[draw, regular polygon, regular polygon sides=6] (L) [above of=g12] {$\oplus$};
	
	\node[draw, rounded corners] (1t) [right = 5cm of 1]{Title};
	\node[draw, rounded corners] (tokenizert) [above = 1cm of 1t] {Tokenizer};
	\node[draw, rounded corners] (2t) [above = 1cm of tokenizert] {word2vec};
	
	\node[draw] (3t) [above of=2t] {$\vec{x}_{t}$};
	\node[draw] (4t) [left of =3t] {$\vec{x}_{t-1}$};
	\node[draw] (5t) [right of =3t] {$\vec{x}_{t+1}$};
	
	\node[draw, circle] (h1t) [above of=3t] {$h^{ t}_1$};
	\node[draw, circle] (h2t) [above of=4t] {$h^{ t-1}_1$};
	\node[draw, circle] (h3t) [above of=5t] {$h^{t+1}_1$};
	
	\node[draw, circle] (g1t) [above of=h1t] {$g^{t}_1$};
	\node[draw, circle] (g2t) [above of=h2t] {$g^{t-1}_1$};
	\node[draw, circle] (g3t) [above of=h3t] {$g^{t+1}_1$};
	
	\node[draw, circle] (h1t2) [above of=g1t] {$h^{ t}_2$};
	\node[draw, circle] (h2t2) [above of=g2t] {$h^{ t-1}_2$};
	\node[draw, circle] (h3t2) [above of=g3t] {$h^{t+1}_2$};
	
	\node[draw, circle] (g1t2) [above of=h1t2] {$g^{t}_2$};
	\node[draw, circle] (g2t2) [above of=h2t2] {$g^{t-1}_2$};
	\node[draw, circle] (g3t2) [above of=h3t2] {$g^{t+1}_2$};
	
	
	\node[draw, regular polygon, regular polygon sides=6] (Lt) [above of=g1t2] {$\oplus$};
	\node[draw, circle] (u) [above right = 3.5cm of L]{$u$};
	\node[draw] (y) [above of = u]{$\hat{y}$};
	
	
	\draw[->] (L) -- (u);
	\draw[->] (Lt) -- (u);
	\draw[->] (u) -- (y);
	
	\draw[->] (1) -- (tokenizer);
	\draw[->] (tokenizer) -- (2);
	\draw[->] (2) -- (3);
	\draw[->] (2) -- (4);
	\draw[->] (2) -- (5);
	
	\draw[->] (3) -- (h1);
	\draw[->] (4) -- (h2);
	\draw[->] (5) -- (h3);
	\draw[->] (h2) -- (h1);
	\draw[->] (h1) -- (h3);
	
	\draw[->] (3)  edge[bend left] (g1);
	\draw[->] (4) edge[bend left] (g2);
	\draw[->] (5) edge[bend left] (g3);
	\draw[->] (g1) -- (g2);
	\draw[->] (g3) -- (g1);
	
	\draw[->] (h1)  edge[bend left] (h12);
	\draw[->] (h2) edge[bend left] (h22);
	\draw[->] (h3) edge[bend left] (h32);
	\draw[->] (h22) -- (h12);
	\draw[->] (h12) -- (h32);
	
	\draw[->] (g1)  edge[bend right] (g12);
	\draw[->] (g2) edge[bend right] (g22);
	\draw[->] (g3) edge[bend right] (g32);
	\draw[->] (g12) -- (g22);
	\draw[->] (g32) -- (g12);
	
	\draw[->] (g22) -- (L);
	\draw[->] (h32) -- (L);
	
	\draw[->] (1t) -- (tokenizert);
	\draw[->] (tokenizert) -- (2t);
	\draw[->] (2t) -- (3t);
	\draw[->] (2t) -- (4t);
	\draw[->] (2t) -- (5t);
	
	\draw[->] (3t) edge[bend right] (h1t);
	\draw[->] (4t) edge[bend right] (h2t);
	\draw[->] (5t) edge[bend right] (h3t);
	\draw[->] (h2t) -- (h1t);
	\draw[->] (h1t) -- (h3t);
	
	\draw[->] (3t)  edge[bend left] (g1t);
	\draw[->] (4t) edge[bend left] (g2t);
	\draw[->] (5t) edge[bend left] (g3t);
	\draw[->] (g1t) -- (g2t);
	\draw[->] (g3t) -- (g1t);
	
	\draw[->] (h1t)  edge[bend left] (h1t2);
	\draw[->] (h2t) edge[bend left] (h2t2);
	\draw[->] (h3t) edge[bend left] (h3t2);
	\draw[->] (h2t2) -- (h1t2);
	\draw[->] (h1t2) -- (h3t2);
	
	\draw[->] (g1t)  edge[bend right] (g1t2);
	\draw[->] (g2t) edge[bend right] (g2t2);
	\draw[->] (g3t) edge[bend right] (g3t2);
	\draw[->] (g1t2) -- (g2t2);
	\draw[->] (g3t2) -- (g1t2);
	
	\draw[->] (g2t2) -- (Lt);
	\draw[->] (h3t2) -- (Lt);
	\end{tikzpicture}
	\caption[Unfolded Model Graph]{Simplified recursive bidirectional Recursive NN (RNN) layout, LSTM connections were removed for clarity. Circles are NN layers, rectangles with curved corners are the preprocessing, hexagons are combined outputs from the RNN layers, and final output and inputs are indicated with squared corners.}\label{rnn}
\end{figure}

 \subsection{Necessity}\label{ness}

Before undertaking the usage of a complex model such as that outlined above, it is worth considering alternatives. There are many other ways to classify text. A Naive Bayes model would generally be the first choice \citep{mccallum1998comparison}, while a K-nearest, SVM, ensemble or logit could also work. Unfortunately none of these models were able to obtain anything better than random guessing and most were worse when a pilot study was performed. The Naive Bayes model was the best performer, arriving at nearly $95\%$ accuracy on the training data, but when compared to the holdout set was worse than random guessing. This pattern of over fitting  or always guessing negative was shown universally in the models selected. Thus a model with a more nuanced understanding of language is required.
\section{Results}


\subsection{Training}

Training the network proved to be more difficult than expected and two separate methods were tried for training the network, one network per subject and one network for the complete dataset. Data prepossessing was time consuming and took about two weeks on high end laptop, data gathering took many hours as well and limitations of the database prevented bibliographic data from being collected too. 

\subsubsection{Per Subject}

To reduce the word embedding run time each subject was separated and the embeddings run independently, with \textit{Law} taking a few hours and \textit{Economics and Business} a couple days to complete. Since the embeddings are independent each subject has to have a separate network trained for it.

To train the networks all the explicitly computational records were combined with a random sample of non-explicit records of twice the size of the explicit set, except in \textit{Media And Communication} where the complete collection was used due to proliferation of computational journals. Then a holdout set of $10\%$ was removed to use for cross validation.

The the models were trained with stochastic gradient descent with the Adam optimizations \citep{collins2012advertiser} and a cross entropy loss, Equation \ref{cross_en} gives the exact formulation used \citep{pytorch}. When training, after every 500 updates to the model (1 epoch), the testing set error was checked. The preferred way to do this is with the loss function, in this case the loss \citep{deeploss} was the cross-entropy of the expected output and the actual output . The cross-entropy  can achieve any positive value, although less than 1 is expected after training. Using average loss to quantify the model's accuracy instead of the raw error rate is preferable as it is less jittery \citep{deeploss} and picks up small improvements (or deteriorations) that are missed with a binary successful/not successful. There are many other measures available, such as the area under the testing receiver operating characteristic curve or the $F_1$ score \citep{james2013introduction}, but those are more useful for tuning the model and many require additional computation.

\begin{align}\label{cross_en}
	loss(\boldsymbol{output}, \boldsymbol{expected}) = - \log \left(\frac{\exp(\boldsymbol{output} \cdot \boldsymbol{expected})}{\sum_j \exp (\boldsymbol{output}_j)}\right)
\end{align}

\begin{figure}[ht]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[width=1.3\textwidth]{loss_mixed}
	\end{adjustbox}
	\caption{Testing loss for all subject's models, across all epochs (1 epoch is 500 training exposures), note that some data for Psychology was lost}\label{loss}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[width=1.3\textwidth]{err_mixed}
	\end{adjustbox}
	\caption{Testing error for all subject's models, across all epochs (1 epoch is 500 training exposures), note that some data for Psychology was lost}\label{err}
\end{figure}

\begin{table}[ht]
	\centering
	\begin{adjustbox}{center}
		\begin{tabular}{lrr}
			\toprule
			{} &  Testing Error &   Loss \\
			\midrule
			Economics And Business  &         \num{0.247} & \num{0.463} \\
			Psychology              &         \num{0.121} & \num{0.272} \\
			Educational Sciences    &         \num{0.173} & \num{0.372} \\
			Sociology               &         \num{0.099} & \num{0.262} \\
			Political Science       &         \num{0.151} & \num{0.458} \\
			Other Social Sciences   &         \num{0.159} & \num{0.339} \\
			Media And Communication &         \num{0.235} & \num{0.493} \\
			Law                     &         \num{0.085} & \num{0.924} \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{The testing loss and error for each subject on the epoch used for the analysis}\label{final_loss}
\end{table}

Figure \ref{loss} shows the loss on the testing set across the epochs for each of subject's models. Notice that most slowly drop by \num{0.1} or so from epoch 0 to epoch 30, at which point they cease decreasing. The plot for \textit{Law}, is not representative of the model's accuracy and the jump is due more to the small testing set size (94) as a couple examples being misclassified will cause a large jump. The loss shown for law, is not the first attempt the parameters. The first run it had the lowest loss and a testing error of 0 for most epochs, which is a sign of over fitting more than of a quality model so it was discarded to try again. Over fitting is a major concern when training neural networks \citep{james2013introduction} and the characteristic switch from nearly flat slope to positive slope is visible in some of the losses, most notably \textit{Political Science} and \textit{Other Social Sciences}.

To help avoid over fitting the testing error rates are also considered, with Figure \ref{err} showing them against epoch. They are nosier than the loss, so they are a secondary consideration as discussed above. In the testing error again around epoch 30 the errors tend to stabilize, so epoch 30 was chose to use for analysing the complete dataset, Figure \ref{final_loss} shows the final values of testing loss and testing error at this epoch. Also note that the error rate for \textit{Law} is quite low despite the high loss, this confirms the small dataset is causing anomalies in the loss.

\subsubsection{Complete}

Once the per subject networks had been trained and tested, training a model on the complete dataset could proceed. A word embedding was created across the whole dataset, which took about a week running on server. Then the explicitly computational records were identified and combined with twice their count in non-computational records and a $10\%$ testing holdout set created. The model was initially tested on a small subset of the testing data, but at epoch 112 this was switched to random samples from the complete set to make sure over fitting was not occurring. The epoch size for this model is 2000 records as it took much longer to converge, and Adam based stochastic gradient descent was still used along with cross-entropy loss. 

Figure \ref{full_loss} shows the loss per epoch of the complete model, the model seems to stop improving around epoch 250 which is \num{35000} more exposures than any of the simple subject networks, thus this was a much slower process taking days on a NVIDIA GeForce GT 750M with \num{2048} MB of RAM. The final model used for this analysis was from epoch 260 as that had particularity good testing error of \num{0.134} and loss of \num{0.316}. Compared to any of the per subject networks it is better than all but \textit{Sociology} and \textit{Law}. Of note is that there were \num{288646} records in the training set, while the model was exposed to \num{520000} records so the model saw each example on average only twice.


\begin{figure}[H]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[width=1.3\textwidth]{full_loss}
	\end{adjustbox}
	\caption{Testing loss for all complete  model, across all epochs (1 epoch is 2000 training exposures), note that at epoch 112 the testing set was expanded}\label{full_loss}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[width=1.3\textwidth]{full_testing_error_rate}
	\end{adjustbox}
	\caption{Testing error rate for all complete  model, across all epochs (1 epoch is 2000 training exposures), note that at epoch 112 the testing set was expanded}\label{full_testing_error_rate}
\end{figure}

\subsection{Word2Vec}

Before considering the results of the models, the validity of the foundations need to be tested. In particular the Word2Vec embedding space is worth some consideration. Using the tags from stack overflow a vocabulary of key words can be constructed. Stack Overflow can be considered to be computational then Economics and Psychology \& Neuroscience are two sets of keywords for non-computational subjects. There is a large bias in Stack Exchange towards computational topics as Stack Overflow was the first and largest site. Thus tags for it Stack Overflow were trimmed to only those with over \num{10000} usages, while the other two sites are unfiltered. Then the tags found in the the Word2Vec embedding space were collected and their embeddings derived, the number of tags used are given in Table \ref{tags}. 

To visualize the distribution of tags a dimension reduction is required, principal component analysis was used to reduce from 200 to 50 dimensions, then t-distributed stochastic neighbor embedding (t-SNE) \citep{maaten2008visualizing} was used to go from 50 to 2. This was done separately for each of the three site's tags, Figure \ref{tsne_indv} and together, Figure \ref{tsne_combined}, Appendix \ref{appendix_tsne} has larger versions of the individual plots. When examined the tags are roughly clustered as expected \texttt{mysql} is near \texttt{database} and \texttt{gdp} is near \texttt{inflation}. But when examined further there are a few oddities \texttt{C} and \texttt{R} are both outliers and not near other languages. Table \ref{nearests} gives the nearested neighbours in the Word2Vec space of a few words. Notice that `C' is near some random strings while `Python' is near other programming languages. This is a byproduct of the tokenization as `C' and `R' are likely incorrectly (or correctly) being separated from equations or initialisms and thus those are the dominant environment, also `C' in particular is part of the copyright symbol. The table shows that other words such as `Sociology' are being correctly placed and thus the embedding as a whole is mostly valid.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[height=1.1\textheight]{tsne_indv}
	\end{adjustbox}
	\caption{t-SNE reduction of tags from Stack Overflow, Economics and Psychology \& Neuroscience, sized by number of usages on the site}\label{tsne_indv}
\end{figure}

\begin{table}[ht]
	\centering
	\begin{adjustbox}{center}
		\begin{tabular}{lrrr}
			\toprule
			Site &  Total Tags &   Used Tags & Top Tags\\
			\midrule
			Stack Overflow &51671&374& \texttt{javascript},\texttt{java},\texttt{php}\\
			Economics &372&275&\texttt{macroeconomics}, \texttt{microeconomics},\texttt{econometrics}\\
			Psychology \& Neuroscience&348&296& \pbox{20cm}{\RaggedLeft\texttt{cognitive-psychology}, \texttt{social-psychology},\\ \texttt{neuroscience}}\\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Stack exchange sites tag usage}\label{tags}
\end{table}

\begin{figure}[H]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[height=.5\textheight]{tsne_combined}
	\end{adjustbox}
	\caption{t-SNE reduction of tags from Stack Overflow, Economics and Psychology \& Neuroscience combined, sized by number of usages on the site, notice some tags occur on multiple sites e.g. statistics}\label{tsne_combined}
\end{figure}

\begin{table}[h]
	\centering
	\begin{adjustbox}{center}
		\begin{tabular}{lllllll}
			\toprule
			{} &           C &      Python &       Sociology &           Statistic &   Computational &            Network \\
			\midrule
			&    \num{358}\num{-379} &         c++ &    anthropology &  kolmogorov-smirnov &      computation &           networks \\
			&  \num{46}:420423 &        perl &     criminology &         chi-squared &        numerical &      network-based \\
			& \num{46}:849\num{-854} &      prolog &       geography &    likelihood-ratio &       simulation &             system \\
			&    \num{267}\num{-276} &        java &      philosophy &                wald &      algorithmic &     networks-based \\
			& \num{53}:566\num{-574} &       c/c++ &    sociological &          chi-square &              cpu &         networking \\
			&    \num{545}\num{-565} &        ampl &      psychology &          two-sample &     mathematical &                hub \\
			& \num{53}:387\num{-391} &  javascript &    epistemology &           bootstrap &  computationally &  experiment-tation \\
			&  :1220\num{-1223} &      opengl &         marxist &           t-student &          runtime &               node \\
			&     xxx-xxx &      matlab &       economics &       durbin-watson &  parallelization &           topology \\
			&    \num{400}\num{-416} &     fortran &  historiography &              f-test &      scalability &              graph \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Selected words and their 10 nearested neighbours}\label{nearests}
\end{table}

\subsection{Comparison}

After training, the models from epoch 30 for each of the subject was selected and ran on all publications in each subject. The resulting percentage of predicted computational publications as compared to the number of explicitly computational is given in Table \ref{ret_sub}. We can see that for all but one subject, Other Social Sciences, there are predicted to be more computational publications than those just published in computational sources. To verify that the predictions are correct samples were collected, both randomly and by selecting those at extrema. To properly do the verification would require multiple coders separately coding hundreds of publications, but to my small sample the results of the analysis look good. Figure \ref{sample_comp} has an example of predicted computational paper from outside the explicitly computational sources, while Figure \ref{sample_not_comp} shows an example of a non-computational paper that the model thinks has signs of being computational. Comparing the two it is evident that the neural network is not just looking for keywords, since the `PORTAL-LIBRARIES AND THE ACADEMY' paper has many `computational' words (see section \ref{word_count} for discussion).

There is also a another model to compare to. The complete model can also produce a table, Table \ref{ret_full}. The table for the complete model disagrees with the partial models somewhat in most cases, but in \textit{Media and Communication} the difference is significant. The extent to which the disagreement is significant can be explored more thoroughly by application of Cohen's $\kappa$ \citep{cohen1960coefficient}. Table \ref{kappas} gives the $\kappa$ values for each subject, the results are more pessimist than the confusion matrices imply, at $0.70$ as the highest value and $0.31$ the lowest. Applying Landis and Koch's well known heuristic \citep{landis1977measurement} we can classify the agreements as fair ($0.31$) to substantial ($0.70$ ). Additionally Cohen's $\kappa$ attempts to down weight based on possible random guessing and as both models are heavily biased towards negative the formula is penalizing the agreement. Thus while not perfect the models can be considerer to have fair agreement, supporting that they are indeed classifying based on the same underlying distribution.

Another way to compare the models is by sampling and comparing manually. Figures \ref{subTrue} and \ref{subFalse} show two papers where the models disagree.

\begin{table}[H]
	\centering
	\begin{adjustbox}{center}
		\begin{tabular}{lrrr}
			\toprule
			{} &  \pbox{20cm}{Percentage\\Explicitly Computational} &  \pbox{20cm}{Percentage\\Predicted Computational} &  Difference \\
			\midrule
			Psychology              &          \num{1.5} &          \num{7.9} &  \num{6.4} \\
			Educational Sciences    &         \num{10.7} &         \num{27.5} & \num{16.8} \\
			Sociology               &          \num{2.9} &          \num{7.1} &  \num{4.2} \\
			Political Science       &          \num{1.9} &         \num{12.4} & \num{10.5} \\
			Other Social Sciences   &          \num{3.4} &         \num{19.5} & \num{16.1} \\
			Media and Communication &         \num{39.0} &         \num{36.1} &  \num{-2.9} \\
			Law                     &          \num{0.8} &          \num{5.7} &  \num{4.9} \\
			Economics and Business  &         \num{13.2} &         \num{36.8} & \num{23.6} \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Comparison ratios of  explicitly computational publications and those predicted to be computational by individual models}\label{ret_sub}
\end{table}

\begin{table}[H]
	\centering
	\begin{adjustbox}{center}
		\begin{tabular}{lrrr}
			\toprule
			{} &   \pbox{20cm}{Percentage\\Explicitly Computational}  &  \pbox{20cm}{Percentage\\Predicted Computational} &  Difference \\
			\midrule
			Psychology              &  \num{1.464321} &  \num{7.052985} &   \num{5.588663} \\
			Educational sciences    & \num{10.685390} & \num{33.249575} &  \num{22.564185} \\
			Sociology               &  \num{2.903707} & \num{11.067687} &   \num{8.163981} \\
			Political science       &  \num{1.892202} & \num{12.397832} &  \num{10.505630} \\
			Other social sciences   &  \num{3.383526} & \num{19.429262} &  \num{16.045736} \\
			Media and communication & \num{38.975246} & \num{62.596464} &  \num{23.621218} \\
			Law                     &  \num{0.766612} &  \num{3.546499} &   \num{2.779887} \\
			Economics and business  & \num{13.173174} & \num{38.588952} &  \num{25.415778} \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Comparison ratios of  explicitly computational publications and those predicted to be computational by the full model, note that counts are slightly different due to differences }\label{ret_full}
\end{table}

\begin{figure}[H]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[width=\textwidth]{full_vs_partial}
	\end{adjustbox}
	\caption{Confusion matrices of full and subject based classifiers showing the normalized concurrence counts}\label{full_vs_partial}
\end{figure}

\begin{table}[H]
	\centering
	\begin{adjustbox}{center}
	\begin{tabular}{cccccccc}
		\toprule
		Psychology & \pbox{20cm}{Educational\\sciences} & Sociology & \pbox{20cm}{Political\\science} & \pbox{20cm}{Other social\\sciences} & \pbox{20cm}{Media and\\communication} & Law & \pbox{20cm}{Economics and\\business}\\
		\midrule
		\num{0.5028116216562004} & \num{0.616736670156338} & \num{0.366795474909127} & \num{0.3062451489371284} & \num{0.38751253484763193} & \num{0.46513269722412487} & \num{0.3142444446243683} & \num{0.7045478875428381}\\
		\bottomrule
	\end{tabular}
\end{adjustbox}
\caption{Cohen's $\kappa$ comparing the full and subject based classifiers}\label{kappas}
\end{table}

\begin{figure}[H]
	\begin{tabular}{ll}
		\toprule
		Field & Value\\
		\midrule
		ID & WOS:000206800000005 \\
		Explicitly Computational & False \\
		Likelyhood is Comp. Full& 45.1\% \\
		Likelyhood is Comp. Subject& 81.98\% \\
		Source &  JASSS-THE JOURNAL OF ARTIFICIAL\\
		& SOCIETIES AND SOCIAL SIMULATION \\
		Year of Publications & 2007.0 \\
		Title &  Higher-Order Simulations: Strategic Investment Under\\
		&Model-Induced Price Patterns \\
		\midrule
		Abstract &  The trading and investment decision processes in financial\\
		&markets become ever more dependent on the use of valuation\\
		&and risk models. In the case of risk management for\\
		&instance, modelling practice has become quite homogeneous\\
		&and the question arises as to the effect this has on the\\
		&price formation process. Furthermore, sophisticated\\
	\end{tabular}
\caption{Example of disagreement between subject and full models}\label{subTrue}
\end{figure}

\begin{figure}[H]
	\begin{tabular}{ll}
		\toprule
		Field & Value\\
		\midrule
		ID & WOS:000207690200005 \\
		Explicitly Computational & False \\
		Likelyhood is Comp.  Full& 60.4\% \\
		Likelyhood is Comp. Subject& 12.28\%\\
		Source & INTERNATIONAL JOURNAL OF HERITAGE STUDIES \\
		Year of Publications & 2008 \\
		Title &  Place as Dialogue: Understanding and Supporting the Museum\\
		&Experience \\
		\midrule
		Abstract &  This paper presents a dialogical approach to place, people\\
		&and technology in museums. The approach has been developed\\
		&in response to concern for locative experience in\\
		&Interaction Design, an approach to the design and\\
		&experience of interactive technologies that emphasises the\\
		&pivotal role played by a wide variety of relationships\\
		&in experience and suggests a set of dimensions of\\
		&experience that have been useful in our interpretations of \dots\\
		\bottomrule
	\end{tabular}
	\caption{Example of disagreement between subject and full models}\label{subFalse}
\end{figure}


\subsection{Introspection}

Another way to examine if the neural network is working the way it is expected to is to examine the output from the RNN section at each word. This is the layers $g^2$ and $h^2$ when they are combined. Since the last layer is a shallow one, we know it is working by taking the weighted sum of their outputs and thus changes the outputs lead to similar changes in the final output. Thus, while we cannot say that the value at index 85 is a significant factor, we can say that a change in many indices is correlated with  indicates a change in the output. Figure \ref{visualize} shows the outputs as the RNN layers read the titles of each of the examples, and a comparison of their final results. By visual inspection we can see that the word `based' in the positive example had a major impact while the colon in the negative started a major change in the negative example. This suggested that the model has identified `based on' as an indicator of computational papers, which is a valid suggestion. While the model has also identified that `: how` is an indicator of a negative example. Unfortunately doing this type of analysis does not have a rigours backing from the literature, work is ongoing though \citep{strobelt2018lstmvis}, as this type of deep introspection is quite new, there will hopefully be better tools developed within the next few years. So while this type of visual can be useful, it cannot currently be the basis of decision making, it is just a tool for inspecting the model.

\begin{figure}[ht]
	\begin{tabular}{ll}
		\toprule
		Field & Value\\
		\midrule
		ID & WOS:000318886700008 \\
		Explicitly Computational & False \\
		Likelyhood is Computational & 81.0\% \\
		Source &  APPLICATION AND BEST PRACTICE OF\\
		&COMPETITIVE TECHNICAL INTELLIGENCE \\
		Subject & Media and Communication\\
		Year of Publications & 2010 \\
		Title &  Research on the Key-technology Selection of Virtual\\
		&Reality Based on Patent Citation Analysis \\
		Citation & \cite{JianmeiWang}\\
		\midrule
		Abstract &  Based on the data of "Derwent Innovation Index" from 1963\\
		&to 2009, the authors construct a patent analysis dataset of\\
		&virtual reality by retrieving the patents through keywords\\
		&and classification numbers. The authors also reveal\\
		&technical hot points, key technologies of virtual reality\\
		&through patent citation analysis and multivariate\\
		&statistical analysis, and then acquire CTI (competitive\\
		&technical intelligence) for enterprises \dots \\
		\bottomrule
	\end{tabular}
\caption{Example of likely computational publication from a non-computational source}\label{sample_comp}
\end{figure}

\begin{figure}[ht]
	\begin{tabular}{ll}
		\toprule
		Field & Value\\
		\midrule
		ID & WOS:000306038900005 \\
		Explicitly Computational & False \\
		Likelyhood is Computational & 19.1\% \\
		Source & PORTAL-LIBRARIES AND THE ACADEMY \\
		Subject & Media and Communication\\
		Year of Publications & 2012 \\
		Title &  Incoming Graduate Students in the Social Sciences: How\\
		&Much Do They Really Know About Library Research? \\
		Citation & \cite{monroe2012incoming}\\
		\midrule
		Abstract &  Academic librarians provide information literacy\\
		&instruction and research services to graduate students. To\\
		&develop evidence-based library instruction and research\\
		&services for incoming graduate students, the authors\\
		&interviewed fifteen incoming graduate students in the\\
		&social sciences and analyzed the interviews using the\\
		&Association of College \& Research Libraries Information\\
		&Literacy Competency Standards for Higher Education (ACRL\\
		&Standards). This article discusses the findings, including\\
		&the authors' assumptions of student information illiteracy,\\
		&trends noted during the interview analysis, and\\
		&implications for delivering information literacy training\\
		&to graduate students in a group discussion modality. \\
		\bottomrule
	\end{tabular}
\caption{Example of possible non-computational publication from a non-computational source}\label{sample_not_comp}
\end{figure}

\begin{figure}[H]
	\centering
		\includegraphics[width=\textwidth]{visualize}
	\caption{RNN activations for each word in two titles; the positive example is on top, the negative below it, and a comparison of each input's final output is shown at the bottom}\label{visualize}
\end{figure}

\subsection{Word Usage}\label{word_count}

Figure \ref{wc} shows a word cloud constructed from the titles of the computational and noncomputational publications in Sociology. The word cloud is constructed by counting the number of occurrences of a word and sizing it according to the count, the words are then laid out to algorithmically. This visualization lets us see what the relationship is between the words used in the two sets. We can see in particular that the computational publications refer to `based' much more than the noncomputational, along with other words such as `Network', `System' and `Information' all of which are related to more analytic approaches. While the noncomputational publications refer to`Relationship', `Social' and `Women' which suggest a more quantitative style of paper. Word clouds for each of the subjects can be found in Appendix \ref{appendix}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{wc_Sociology}
	\caption{Word cloud of Sociology publication's titles}\label{wc}
\end{figure}

\subsection{Temporal Effects}

Figure \ref{temporal} show the number of computational papers per year for each of the subjects. The peak around 2007 for Media and Communications, and a couple others, is mostly due to some conferences (`International Conference on Information Management, Innovation Management and Industrial Engineering' contributes the most) being reclassified from Media and Communications to either a pure computer science or a more general subject (e.g. `Multidisciplinary Sciences') after 2008. What is notable is that the number of computational papers indicated by the model do follow the curve, but with a longer delay. Even as the conferences left the need to publish remained. Additionally of note, is the low level of computational work that is present in all disciplines. These are publications like `The application of virtual reality technology in interior design system development' \citep{chuanrong2016application}, which are discussions of computational techniques by members of the community for the community. In some cases they are like Boyle \citep{shapin1985leviathan}, attempting to layout a new programme of study to a sceptical audience who do not have the capabilities to observe the phenomena first hand. While in others the publications are completely integrated into the existing culture and communicating discoveries or designs of machines that are understood by the community \citep{cetina2009epistemic}. Distinguishing between the two classes of publications is a challenge that will likely be overcome at a later date, but for now I can say only that it seems social scientists have not changed their discussion of computational  techniques by a significant amount in last decade, baring some decreases in Psychology, Sociology and Economics.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[width=1.2\textwidth]{temporal}
	\end{adjustbox}
	\caption{Yearly percentages of explicitly computational and predicted computational papers, notice the higher base line for the predicted papers and the smoother yearly transitions }\label{temporal}
\end{figure}

\subsection{Authors}

Figure \ref{auths} shows the smoothed probability density of the number of publications per author across computational and noncomputational publications. The number of publications by each author was counted, then their counts were distributed from 0 to 20, with higher counts being cut down to 20 (thus the bump for a few subjects). Then a kernel density estimator with a bandwidth of .5 was used to interpolate and smooth, giving the continuous probability densities displayed.

The difference between the peaks of most computational and noncomputational is not surprising, most Psychologists aren't writing software. What is more surprising is the ones with overlap. High degrees of overlap suggests computational approaches are an excepted and normal means of doing science for that community, maybe these computational people are forming their own community with it's own mediums of exchange with the outside world \citep{star1989institutional} or they are integrated into the knowledge society \citep{cetina2009epistemic}. More research on the subject would be required to answer this.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{center}
		\includegraphics[width=1.2\textwidth]{auths_dist}
	\end{adjustbox}
	\caption{Probability density of the number of publication for a random author from either the computational, noncomputational or combined pool of publications in a subject}\label{auths}
\end{figure}


\section{Discussion}

\subsection{What is being classified}

Unlike the more traditional techniques discussed in Section \ref{ness}, deep neural networks' decision making processes are not well understood \citep{deep_learning_chapter12}. This limits understanding to inferences based on the observed outputs and inputs. The nominal goal of the classifier describe above is to identify computational usage across disciplines. But that is not what the training and testing data are measuring against. The data provided are collections of works from selected journals. What the journals impose on the papers, in particular upon their abstracts, that is different from the non-computational journals is what the models are trained to identify. While there are certainly a much higher percentage of computational papers in the journals, as was shown above they are not the sole holders of the computational keys. It is likely more accurate to characterize the classifications to being derived from the style of the abstract and title than the substance.

Then again, what makes a paper computational if not the style. Computer science existed well before computers could be produced, e.g. Leibniz's difference engine or Ada Lovelace's Note G. It could be considered that computational social science is as much a style of work as it is a set of methods. In \textit{Bit by Bit} Matthew Salganik \citep{salganik2017bit} describes many types of computational experiment, but the analysis required for some of them would not be unfamiliar to Kurt Lewin \citep{lewin1939field} all that changes is that the data are collected without the experimenter leaving their lab. What make a research topic computational is the style of the approach, collection of all data possible and maybe more complex modelling at the end, not the substance. Thus looking for computational style might be the correct way to consider the problem of discovering computational usage.

\subsection{Results}

These results are not the definitive word on the distribution of computational methods across the social sciences, they are though a starting point. That this type of analysis produces anything but noise shows that the deep neural network approach to complex, unstructured text is worth exploring and I plan to continue with it, hopefully improving accuracy and definitely better mapping out how long these tasks will take. 

In addition to the improvements there are a two problems with the current analysis that will need to be addressed. First the usage of journals as the basis of the initial classification is very coarse, either a better method needs to be created or more human coding must be done. The unfortunately for the former method a simple heuristic or machine learning approach can't be used as then the neural network would learn that instead of the correct distinction \citep{deep_learning_chapter12}. Thus my preferred solution is to reduce the training time and train a series of networks, with the outputs from each generation being used to enrich the next's training data. This has the downside of requiring  a large amount of human intervention though. The other issue with this result is that it cannot be generalize. The usage of word embedding trained on the complete universe of interest means that if a new record is to be considered the embedding has to be redone, or at least updated, which in either case requires a complete retraining of the model. Even with these two issues there are some useful results, and both can be mediated with further work.
\subsection{Further Steps}

The baseline activity in computation across all subjects is  very intriguing, as it suggests that computational approaches may not be as divisive as some suggest \citep{watts2007twenty}\citep{lazer2009life}. There is already precedent for scientists to accept and use new tools, machinery or ideas, without having to change their paradigm. The Latourian idea of a black box \citep{latour1987science} can be used to describe many computational techniques. Do everyday scientists care how the the line of the linear regression is derived? No, they only care that it is accurate, so why would it matter if a grad student or a computer made it? The techniques may be more complicated than a linear regression, but that may require a new sex \citep{collins1975seven}, focused on complex computational systems to be need for replication, but will it require a fundamental shift in methodology? 

An additional point of particular interest that this would form the basis of answering, is `how do new styles/methods diffuse through science?' There is much work already on diffusion of knowledge \citep{griliches1960hybrid} \citep{crane1972invisible} \citep{evans2010industry}, but there is much less work on diffusion of style or technique. The data provided by this method combined with bibliographic analysis would likely reveal some interesting results. Additionally the method is not limited to identifying computational approaches it can be trivially generalized given the correct training data. Of note instead of binary classification, a mixture model could be created, where different styles exist in different proportions in each record thus along the competition of styles to be considered.

There are few more aspects, particular to Science Studies, that were not able to be a part of this analysis. First, examining the impact of gender on computational publications, women's representation in software development is low is also low in scientific computation? Secondly how do computational researchers fit into the modes of science already in place, e.g. can computational sociology coexist with the strong programme \citep{bloor1976strong}? This work suggests that they can fit in well. Finally is the advent of computational social science a paradigm shift? Or is just another set of black box to help scientists to get slightly closer to the truth?

\newpage
\singlespacing
\setcounter{page}{1}
\pagenumbering{roman}
\addcontentsline{toc}{section}{References}
\bibliography{Report}{}

\bibliographystyle{asr}


\appendix
\section{Appendix: Word Clouds}\label{appendix}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{wc_Economics_And_Business}
	\caption{Word cloud of Economics And Business publication's titles}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{wc_Psychology}
	\caption{Word cloud of Psychology publication's titles}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{wc_Educational_Sciences}
	\caption{Word cloud of Educational Sciences publication's titles}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{wc_Sociology}
	\caption{Word cloud of Sociology publication's titles}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{wc_Political_Science}
	\caption{Word cloud of Political Science publication's titles}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{wc_Other_Social_Sciences}
	\caption{Word cloud of Other Social Sciences publication's titles}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{wc_Media_And_Communication}
	\caption{Word cloud of Media And Communication publication's titles}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{wc_Law}
	\caption{Word cloud of Law publication's titles}
\end{figure}
\section{Appendix: Word2Vec t-SNE}\label{appendix_tsne}
\begin{figure}[ht]
	\centering
	\includegraphics[height=\textheight]{tsne_psychologyneuroscience}
	\caption{t-SNE layout of Tags from Psychology \& Neuroscience}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[height=\textheight]{tsne_economics}
	\caption{t-SNE layout of Tags from Economics}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[height=\textheight]{tsne_stack_overflow}
	\caption{t-SNE layout of Tags from Stack Overflow}
\end{figure}
\end{document}